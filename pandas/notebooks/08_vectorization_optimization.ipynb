{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc63b09e-22a2-4a09-a084-2bb89f53a474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Plan catalog ----------\n",
    "plans = pd.DataFrame({\n",
    "    \"plan\": [\"Basic\",\"Standard\",\"Plus\",\"Pro\",\"Ultra\"],\n",
    "    \"base_fee\": [25.0, 40.0, 55.0, 70.0, 85.0],\n",
    "    \"min_included\": [300, 600, 800, 1000, 1200],\n",
    "    \"data_included\": [3.0, 6.0, 8.0, 10.0, 12.0],\n",
    "    \"min_overage_rate\": [0.08, 0.05, 0.04, 0.03, 0.02],\n",
    "    \"data_overage_rate\": [8.0, 6.0, 5.0, 4.0, 3.5],\n",
    "})\n",
    "\n",
    "# ---------- Usage facts ----------\n",
    "n = 100_000  # >= 100k as required\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "plan_probs = [0.35, 0.35, 0.15, 0.10, 0.05]  # skew toward Basic/Standard\n",
    "region_vals = np.array([\"West\",\"East\",\"South\",\"North\",\"Central\"])\n",
    "region_probs = [0.25, 0.25, 0.20, 0.20, 0.10]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"customer_id\": np.arange(1, n+1, dtype=np.int64),\n",
    "    \"plan\": rng.choice(plans[\"plan\"].values, size=n, p=plan_probs),\n",
    "    \"minutes_used\": np.clip(rng.normal(loc=600, scale=200, size=n).round().astype(np.int64), 0, None),\n",
    "    \"data_gb\": np.clip(rng.normal(loc=8.0, scale=4.0, size=n), 0, None).astype(float),\n",
    "    \"is_active\": rng.random(n) < 0.85,\n",
    "    \"region\": rng.choice(region_vals, size=n, p=region_probs),\n",
    "})\n",
    "\n",
    "# Sanity checks (optional)\n",
    "assert set(df[\"plan\"].unique()).issubset(set(plans[\"plan\"].unique()))\n",
    "assert df.shape[0] >= 100_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ce486fe-6ea7-4e6e-a6a3-558dc357a918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old pipeline input rows (pre-filter): 100,000\n",
      "Old pipeline rows after filter:      21,389\n",
      "Old pipeline runtime:                0.433 s\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "# Build a plan lookup dict for row-wise access (still \"old\" since we loop via apply)\n",
    "plan_lookup = plans.set_index(\"plan\")[[\n",
    "    \"base_fee\",\"min_included\",\"data_included\",\"min_overage_rate\",\"data_overage_rate\"\n",
    "]].to_dict(\"index\")\n",
    "\n",
    "def compute_monthly_bill_row(row):\n",
    "    p = row[\"plan\"]\n",
    "    attrs = plan_lookup[p]\n",
    "    base = attrs[\"base_fee\"]\n",
    "    min_inc = attrs[\"min_included\"]\n",
    "    dat_inc = attrs[\"data_included\"]\n",
    "    min_rate = attrs[\"min_overage_rate\"]\n",
    "    dat_rate = attrs[\"data_overage_rate\"]\n",
    "\n",
    "    min_over = max(row[\"minutes_used\"] - min_inc, 0)\n",
    "    dat_over = max(row[\"data_gb\"] - dat_inc, 0.0)\n",
    "    return base + min_over * min_rate + dat_over * dat_rate\n",
    "\n",
    "# Baseline timing\n",
    "t0 = perf_counter()\n",
    "\n",
    "df_old = df.copy()  # operate on full data (no predicate pushdown)\n",
    "df_old[\"monthly_bill\"] = df_old.apply(compute_monthly_bill_row, axis=1)\n",
    "\n",
    "# NOW filter (intentionally bad ordering for performance)\n",
    "df_old = df_old[(df_old[\"is_active\"]) & (df_old[\"region\"] == \"West\")]\n",
    "\n",
    "# Compute region total the slow way and merge back (not transform)\n",
    "region_totals = (\n",
    "    df_old.groupby(\"region\", as_index=False)[\"monthly_bill\"]\n",
    "          .sum()\n",
    "          .rename(columns={\"monthly_bill\": \"region_bill_total\"})\n",
    ")\n",
    "df_old = df_old.merge(region_totals, on=\"region\", how=\"left\")\n",
    "df_old[\"customer_share\"] = df_old[\"monthly_bill\"] / df_old[\"region_bill_total\"]\n",
    "\n",
    "t_old = perf_counter() - t0\n",
    "\n",
    "print(f\"Old pipeline input rows (pre-filter): {len(df):,}\")\n",
    "print(f\"Old pipeline rows after filter:      {len(df_old):,}\")\n",
    "print(f\"Old pipeline runtime:                {t_old:.3f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b2110bd5-576a-4048-8a99-2dc21e7bbd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New pipeline input rows (pre-filter): 100,000\n",
      "New pipeline rows after filter:      21,389\n",
      "New pipeline runtime:                0.037 s\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "t0 = perf_counter()\n",
    "\n",
    "mask = df[\"is_active\"] & (df[\"region\"] == \"West\")\n",
    "df_new = df.loc[mask].copy()\n",
    "\n",
    "plan_cols = plans[['plan', 'base_fee', 'min_included', 'data_included', 'min_overage_rate', 'data_overage_rate']]\n",
    "\n",
    "df_new = df_new.merge(plan_cols, on='plan', how='left', validate=\"m:1\", indicator=True)\n",
    "\n",
    "# 1) Inspect join result\n",
    "# print(df_new[\"_merge\"].value_counts(dropna=False))\n",
    "# results were clean, only on both, 0 on left or right\n",
    "\n",
    "# 2) Hard-fail if any unmatched usage rows\n",
    "if (df_new[\"_merge\"] != \"both\").any():\n",
    "    missing_keys = df_new.loc[df_new[\"_merge\"] == \"left_only\", \"plan\"].unique()\n",
    "    raise ValueError(f\"Unmatched plan keys in usage: {missing_keys}\")\n",
    "\n",
    "req = [\"base_fee\",\"min_included\",\"data_included\",\"min_overage_rate\",\"data_overage_rate\"]\n",
    "\n",
    "bad = df_new[req].isna().any(axis=1)\n",
    "if bad.any():\n",
    "    # Diagnostics\n",
    "    print(\"Nulls by column:\\n\", df_new.loc[bad, req].isna().sum())\n",
    "    bad_plans = df_new.loc[bad, \"plan\"].unique()\n",
    "    raise ValueError(f\"Joined attributes contain NaN for plans: {bad_plans}\")\n",
    "\n",
    "df_new = df_new.drop(columns=\"_merge\")\n",
    "\n",
    "\n",
    "\n",
    "# Ensure usage columns are numeric too\n",
    "df_new[\"minutes_used\"] = pd.to_numeric(df_new[\"minutes_used\"], errors=\"raise\")\n",
    "df_new[\"data_gb\"]      = pd.to_numeric(df_new[\"data_gb\"], errors=\"raise\")\n",
    "\n",
    "df_new = df_new.astype({\n",
    "    \"base_fee\": \"float64\",\n",
    "    \"min_included\": \"int64\",           \n",
    "    \"data_included\": \"float64\",\n",
    "    \"min_overage_rate\": \"float64\",\n",
    "    \"data_overage_rate\": \"float64\",\n",
    "})\n",
    "\n",
    "\n",
    "# vectorize after a merge on plan\n",
    "df_new['monthly_bill'] = df_new['base_fee'] + (df_new['minutes_used'] - df_new['min_included']).clip(lower=0) * df_new['min_overage_rate'] + (df_new['data_gb'] - df_new['data_included']).clip(lower=0) * df_new['data_overage_rate']\n",
    "\n",
    "\n",
    "# Per-region total via transform (no extra merge/materialization)\n",
    "df_new[\"region_bill_total\"] = df_new.groupby(\"region\")[\"monthly_bill\"].transform(\"sum\")\n",
    "df_new[\"customer_share\"] = df_new[\"monthly_bill\"] / df_new[\"region_bill_total\"]\n",
    "\n",
    "assert (df_new[\"monthly_bill\"].notna()).all(), \"monthly_bill has NaNs\"\n",
    "assert (df_new[\"monthly_bill\"] >= df_new[\"base_fee\"]).all(), \"bill < base_fee found\"\n",
    "\n",
    "t_new = perf_counter() - t0\n",
    "\n",
    "print(f\"New pipeline input rows (pre-filter): {len(df):,}\")\n",
    "print(f\"New pipeline rows after filter:      {len(df_new):,}\")\n",
    "print(f\"New pipeline runtime:                {t_new:.3f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bc686aef-5eb5-4ea2-a9a9-fac78794fa7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical cast (region) applied to shared df:\n",
      "  region bytes  before: 100,614\n",
      "  region bytes   after: 100,614\n",
      "  frame  bytes  before: 8,871,014\n",
      "  frame  bytes   after: 8,871,014\n",
      "  region savings: 0 bytes\n",
      "  frame  savings: 0 bytes\n",
      "\n",
      "=== Benchmark (same input) ===\n",
      "Rows in:         100,000\n",
      "Rows post-filter (old):  21,389\n",
      "Rows post-filter (push): 21,389\n",
      "Rows post-filter (vec):  21,389\n",
      "\n",
      "Runtimes:\n",
      "  Old baseline (apply then filter): 0.412 s\n",
      "  Pushdown + apply:                  0.089 s\n",
      "  Vectorized:                        0.009 s\n",
      "\n",
      "Speedups (higher is better):\n",
      "  Old / Vectorized:       46.22×\n",
      "  Push+Apply / Vectorized:10.03×\n",
      "\n",
      "_merge status (vectorized): {'both': 21389, 'left_only': 0, 'right_only': 0}\n",
      "\n",
      "Equivalence spot-check on 20 rows: 20/20 exact within 1e-9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pn/q39n4_bd7rq84krr4s_56sn00000gn/T/ipykernel_32578/1970371912.py:52: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  region_totals = (df_old.groupby(\"region\", as_index=False)[\"monthly_bill\"]\n",
      "/var/folders/pn/q39n4_bd7rq84krr4s_56sn00000gn/T/ipykernel_32578/1970371912.py:64: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df2[\"region_bill_total\"] = df2.groupby(\"region\")[\"monthly_bill\"].transform(\"sum\")\n",
      "/var/folders/pn/q39n4_bd7rq84krr4s_56sn00000gn/T/ipykernel_32578/1970371912.py:113: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  out[\"region_bill_total\"] = out.groupby(\"region\")[\"monthly_bill\"].transform(\"sum\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import perf_counter\n",
    "\n",
    "# --- 0) Prereqs: row-wise function (baseline uses it) -------------------------\n",
    "# If you already defined plan_lookup and compute_monthly_bill_row, reuse them.\n",
    "plan_lookup = plans.set_index(\"plan\")[[\n",
    "    \"base_fee\",\"min_included\",\"data_included\",\"min_overage_rate\",\"data_overage_rate\"\n",
    "]].to_dict(\"index\")\n",
    "\n",
    "def compute_monthly_bill_row(row):\n",
    "    p = row[\"plan\"]\n",
    "    attrs = plan_lookup[p]\n",
    "    base = attrs[\"base_fee\"]\n",
    "    min_inc = attrs[\"min_included\"]\n",
    "    dat_inc = attrs[\"data_included\"]\n",
    "    min_rate = attrs[\"min_overage_rate\"]\n",
    "    dat_rate = attrs[\"data_overage_rate\"]\n",
    "    min_over = max(row[\"minutes_used\"] - min_inc, 0)\n",
    "    dat_over = max(row[\"data_gb\"] - dat_inc, 0.0)\n",
    "    return base + min_over * min_rate + dat_over * dat_rate\n",
    "\n",
    "# --- 1) Region -> category + memory effect -----------------------------------\n",
    "\n",
    "# --- 1) Region -> category + memory effect (applied to real df) ---\n",
    "mem_before_frame  = df.memory_usage(deep=True).sum()\n",
    "mem_before_region = df[\"region\"].memory_usage(deep=True)\n",
    "\n",
    "df = df.copy()\n",
    "df[\"region\"] = df[\"region\"].astype(\"category\")\n",
    "\n",
    "mem_after_frame   = df.memory_usage(deep=True).sum()\n",
    "mem_after_region  = df[\"region\"].memory_usage(deep=True)\n",
    "\n",
    "print(\"Categorical cast (region) applied to shared df:\")\n",
    "print(f\"  region bytes  before: {mem_before_region:,}\")\n",
    "print(f\"  region bytes   after: {mem_after_region:,}\")\n",
    "print(f\"  frame  bytes  before: {mem_before_frame:,}\")\n",
    "print(f\"  frame  bytes   after: {mem_after_frame:,}\")\n",
    "print(f\"  region savings: {mem_before_region - mem_after_region:,} bytes\")\n",
    "print(f\"  frame  savings: {mem_before_frame  - mem_after_frame:,} bytes\")\n",
    "\n",
    "\n",
    "\n",
    "# --- 2) Pipelines -------------------------------------------------------------\n",
    "\n",
    "def pipeline_old_apply_then_filter(df):\n",
    "    t0 = perf_counter()\n",
    "    df_old = df.copy()\n",
    "    df_old[\"monthly_bill\"] = df_old.apply(compute_monthly_bill_row, axis=1)\n",
    "    df_old = df_old[(df_old[\"is_active\"]) & (df_old[\"region\"] == \"West\")].copy()\n",
    "    region_totals = (df_old.groupby(\"region\", as_index=False)[\"monthly_bill\"]\n",
    "                           .sum()\n",
    "                           .rename(columns={\"monthly_bill\": \"region_bill_total\"}))\n",
    "    df_old = df_old.merge(region_totals, on=\"region\", how=\"left\")\n",
    "    df_old[\"customer_share\"] = df_old[\"monthly_bill\"] / df_old[\"region_bill_total\"]\n",
    "    t = perf_counter() - t0\n",
    "    return df_old, t\n",
    "\n",
    "def pipeline_pushdown_then_apply(df):\n",
    "    t0 = perf_counter()\n",
    "    df2 = df.loc[df[\"is_active\"] & (df[\"region\"] == \"West\")].copy()\n",
    "    df2[\"monthly_bill\"] = df2.apply(compute_monthly_bill_row, axis=1)\n",
    "    df2[\"region_bill_total\"] = df2.groupby(\"region\")[\"monthly_bill\"].transform(\"sum\")\n",
    "    df2[\"customer_share\"] = df2[\"monthly_bill\"] / df2[\"region_bill_total\"]\n",
    "    t = perf_counter() - t0\n",
    "    return df2, t\n",
    "\n",
    "def pipeline_vectorized(df, plans):\n",
    "    t0 = perf_counter()\n",
    "    # filter early\n",
    "    base = df.loc[df[\"is_active\"] & (df[\"region\"] == \"West\"),\n",
    "                  [\"region\",\"plan\",\"minutes_used\",\"data_gb\",\"is_active\"]].copy()\n",
    "\n",
    "    # minimal plan columns, constrained merge + indicator for diagnostics\n",
    "    plan_cols = plans[[\"plan\",\"base_fee\",\"min_included\",\"data_included\",\n",
    "                       \"min_overage_rate\",\"data_overage_rate\"]]\n",
    "    out = base.merge(plan_cols, on=\"plan\", how=\"left\", validate=\"m:1\", indicator=True)\n",
    "\n",
    "    # join diagnostics\n",
    "    merge_counts = out[\"_merge\"].value_counts(dropna=False).to_dict()\n",
    "    if (out[\"_merge\"] != \"both\").any():\n",
    "        bad_keys = out.loc[out[\"_merge\"]==\"left_only\",\"plan\"].unique()\n",
    "        raise ValueError(f\"Unmatched plan keys in usage: {bad_keys}\")\n",
    "    out = out.drop(columns=\"_merge\")\n",
    "\n",
    "    # required columns present\n",
    "    req = [\"base_fee\",\"min_included\",\"data_included\",\"min_overage_rate\",\"data_overage_rate\"]\n",
    "    bad = out[req].isna().any(axis=1)\n",
    "    if bad.any():\n",
    "        bad_plans = out.loc[bad,\"plan\"].unique()\n",
    "        raise ValueError(f\"Joined attributes contain NaN for plans: {bad_plans}\")\n",
    "\n",
    "    # numeric dtypes\n",
    "    out[\"minutes_used\"] = pd.to_numeric(out[\"minutes_used\"], errors=\"raise\")\n",
    "    out[\"data_gb\"]      = pd.to_numeric(out[\"data_gb\"], errors=\"raise\")\n",
    "    out = out.astype({\n",
    "        \"base_fee\": \"float64\",\n",
    "        \"min_included\": \"int64\",\n",
    "        \"data_included\": \"float64\",\n",
    "        \"min_overage_rate\": \"float64\",\n",
    "        \"data_overage_rate\": \"float64\",\n",
    "    })\n",
    "\n",
    "    # vectorized overage math\n",
    "    min_over  = (out[\"minutes_used\"] - out[\"min_included\"]).clip(lower=0)\n",
    "    data_over = (out[\"data_gb\"]      - out[\"data_included\"]).clip(lower=0)\n",
    "    out[\"monthly_bill\"] = (out[\"base_fee\"]\n",
    "                           + min_over  * out[\"min_overage_rate\"]\n",
    "                           + data_over * out[\"data_overage_rate\"])\n",
    "\n",
    "    # region totals + share with zero-guard\n",
    "    out[\"region_bill_total\"] = out.groupby(\"region\")[\"monthly_bill\"].transform(\"sum\")\n",
    "    zero_tot = out[\"region_bill_total\"] == 0\n",
    "    out[\"customer_share\"] = np.where(zero_tot, 0.0, out[\"monthly_bill\"] / out[\"region_bill_total\"])\n",
    "\n",
    "    # invariants\n",
    "    assert out[\"monthly_bill\"].notna().all()\n",
    "    assert (out[\"monthly_bill\"] >= out[\"base_fee\"]).all()\n",
    "\n",
    "    t = perf_counter() - t0\n",
    "    return out, t, merge_counts\n",
    "\n",
    "# --- 3) Run all on same input size (>=100k) -----------------------------------\n",
    "# Ensure df has >=100k rows; if not, your numbers will differ from your target scale.\n",
    "\n",
    "input_rows = len(df)\n",
    "old_df,  t_old  = pipeline_old_apply_then_filter(df)\n",
    "push_df, t_push = pipeline_pushdown_then_apply(df)\n",
    "vec_df,  t_vec, merge_counts = pipeline_vectorized(df, plans)\n",
    "\n",
    "# --- 4) Benchmark table + speedups -------------------------------------------\n",
    "def fmt(s): \n",
    "    return f\"{s:,.3f} s\"\n",
    "\n",
    "print(\"\\n=== Benchmark (same input) ===\")\n",
    "print(f\"Rows in:         {input_rows:,}\")\n",
    "print(f\"Rows post-filter (old):  {len(old_df):,}\")\n",
    "print(f\"Rows post-filter (push): {len(push_df):,}\")\n",
    "print(f\"Rows post-filter (vec):  {len(vec_df):,}\")\n",
    "print(\"\\nRuntimes:\")\n",
    "print(f\"  Old baseline (apply then filter): {fmt(t_old)}\")\n",
    "print(f\"  Pushdown + apply:                  {fmt(t_push)}\")\n",
    "print(f\"  Vectorized:                        {fmt(t_vec)}\")\n",
    "\n",
    "print(\"\\nSpeedups (higher is better):\")\n",
    "print(f\"  Old / Vectorized:       {t_old / t_vec:,.2f}×\")\n",
    "print(f\"  Push+Apply / Vectorized:{t_push / t_vec:,.2f}×\")\n",
    "\n",
    "print(\"\\n_merge status (vectorized):\", merge_counts)  # expect {'both': post-filter rows}\n",
    "\n",
    "# --- 5) Equivalence spot-check (apply vs vectorized on a sample) -------------\n",
    "# Sample 10–20 rows from the *vectorized* result and recompute the apply math for those same rows.\n",
    "n = min(20, len(vec_df))\n",
    "sample = vec_df.sample(n=n, random_state=42)\n",
    "\n",
    "# Rebuild a small frame with the columns the row-function expects\n",
    "check = sample[[\"plan\",\"minutes_used\",\"data_gb\"]].copy()\n",
    "# The row-wise function uses plan_lookup dict built from `plans` above\n",
    "apply_vals = check.apply(compute_monthly_bill_row, axis=1).values\n",
    "vec_vals   = sample[\"monthly_bill\"].values\n",
    "\n",
    "equal_mask = np.isclose(apply_vals, vec_vals, rtol=0, atol=1e-9)\n",
    "print(f\"\\nEquivalence spot-check on {n} rows: {equal_mask.sum()}/{n} exact within 1e-9\")\n",
    "if not equal_mask.all():\n",
    "    diffs = pd.DataFrame({\n",
    "        \"plan\": sample[\"plan\"].values,\n",
    "        \"apply\": apply_vals,\n",
    "        \"vectorized\": vec_vals,\n",
    "        \"delta\": vec_vals - apply_vals\n",
    "    })\n",
    "    print(\"Mismatches (head):\")\n",
    "    print(diffs.loc[~equal_mask].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aa5fa2-9318-4b36-bcb5-e629ec44cb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb170ef0-3254-424b-999f-94bde4800b97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
